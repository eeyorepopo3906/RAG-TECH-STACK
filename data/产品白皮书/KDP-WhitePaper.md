---
title: Kubernetes大数据平台产品白皮书
date: 2023-12-20 18:17:10
tag: v4.2
category: [产品文档, v4.2]
---

# Kubernetes大数据平台产品白皮书

Kubernetes Data Platform White Paper

# 1 前言

以Hadoop为中心的大数据生态系统从2006年开源以来，一直是大部分公司构建大数据平台的选择，但这种传统选择随着人们深入地使用，出现越来越多的问题，比如：系统组件安装配置复杂、集群资源利用效率低、运维工作量大、数据应用开发迭代效率低、新的开发工具集成非常复杂。这些问题已经成为了困扰企业数字化转型加速迭代和升级的重要障碍。特别是在越来越多的企业将业务系统负载逐渐进行云原生改造，迁移到基于Kubernetes的私有或公有云平台上之后，在云原生体系之外独立运行一套传统大数据平台在架构上和运营上又增加了不少不必要的复杂度和资源浪费。

Kubernetes大数据平台（以下简称KDP）是智领云科技有限公司 （ LinkTime Cloud ） 自主研发的基于云原生架构的大数据平台。该平台采用Kubernetes（以下简称K8s）作为资源调度平台，统一调度和管理大数据组件以及数据应用。在对开源的大数据计算及存储引擎的改造和集成基础之上，通过智领云自研的大数据集成基座，该平台实现了以标准的方式来部署、发布、管理和运维主流大数据组件。平台上所有大数据组件均纳入了统一的监控报警和日志管理的可观测体系，帮助大数据开发者即时发现故障并快速定位故障发生原因。同时，平台提供企业级的多租户安全认证和权限管理机制，保证了数据的安全和可靠性。

智领云是数字化运营的IT架构赋能者，团队成员来自Ask.com、Twitter、EA（艺电）、IBM 、ThoughtWorks、惠普、华为、爱立信、京东等国内外知名高科技企业，公司自主研发的BDOS云原生大数据系列产品能帮助企业快速搭建数据和AI中台，实现数据的打通、共享和复用，轻松打造业务数据能力闭环，掌握全面、及时、更多维度的业务现状，提升数据驱动应用的迭代和发布速度， 高效地实现企业数字化运营。 BDOS 云原生大数据系列产品已在医疗， 政务，物联网，教育，金融，能源多个行业落地，并获得了客户的一致好评。KDP作为BDOS的云原生架构底座，可以帮助用户快速在K8s上搭建全新云原生架构的大数据平台，也可以将传统大数据平台无缝迁移到K8s上，打造新一代数据驱动的IT架构。

# 2 为什么要选择云原生大数据技术路线

## 2.1 传统大数据平台的问题

传统大数据平台，是指的以Hadoop为中心的大数据生态技术。一个Hadoop集群包含HDFS分布式文件系统和以Yarn为调度系统的MapReduce计算框架。围绕Hadoop，有一系列的软件来帮助人们进行大数据的数据采集、存储和计算、以及数据应用的开发，比如数据采集Sqoop、日志采集Flume、数据仓库Hive、计算框架Spark、实时消息队列Kafka、任务调度系统、监控系统、数据立方体Kylin等。下图所示就是一个典型的传统大数据平台架构，一般目前使用的大数据平台都是类似的架构，采用的大数据组件可能会有所区别。

<img src="KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-47-45.png" />

在大数据发展的初期，这样的大数据生态技术框架是能基本满足企业和机构建设大数据平台的需要的。随着时代的发展，大数据技术使用逐步地深入，大数据开发需求变得越来越旺盛，企业对多租户环境下大数据开发的效率、大数据集群资源利用率、新的计算存储引擎、人工智能和机器学习技术的集成速度提出了越来越高的要求，而传统大数据平台在面对这些需求时则显得有点束手无策，出现了无法依靠自身的发展来克服这些困难的困境。我们仔细分析一下，就可以看到传统大数据平台的技术架构决定了依靠它本身的发展是无法克服这些困难的：

- 困难一：传统大数据平台难以实现资源的隔离。多租户环境下的数据开发效率提升，需要以资源隔离的方式来保证租户之间的计算作业互相不影响，特别是不能出现某一个或几个租户独占集群资源的情况。但Hadoop系统本身的出发点就不是为了多租户环境而设计的，其目前的资源隔离实现也不完善。在最近的Hadoop版本中，在一定程度上实现了内存资源和文件资源的隔离，但是不够完整，而磁盘I/O和网络I/O的隔离还在社区讨论的过程中，短期内看不到解决的希望。
- 困难二：传统大数据平台难以集成新的计算和存储技术。Hadoop系统在部署其他组件的时候，对这些组件与HDFS和Yarn的版本适配是有严格要求的。很多新的大数据组件是不适配老版本的Hadoop的，而升级Hadoop又会造成其他组件的失效。另外，部署新的组件还要考虑到Linux不同操作系统的兼容性所带来的额外复杂度。所以引入一个新的计算和存储组件的难度是非常高的，往往需要几天甚至是几周的时间。
- 困难三：Hadoop存算合一的紧耦合架构决定了它的资源利用率无法提高。在一个Hadoop集群中，一个节点既是存储节点（data node）， 也是计算节点。 当存储资源不够的时候， 增加节点可以进行存储扩容，但会造成计算资源的利用率下降；同样，当计算资源不够而进行扩容的时候，存储资源利用率就会下降。同时，因为对于Yarn的依赖，不使用Yarn调度的其它组件很难集成到Hadoop的计算框架中。所以Hadoop的这种耦合架构决定了它的资源利用率不高。
- 困难四：Hadoop集群资源无法做到快速的弹性扩容和缩容。弹性的扩容和缩容是提高集群资源利用率的有效方法。很遗憾，Hadoop的节点扩容和缩容流程，导致这个动作无法在很快的时间内完成，尤其是缩容过程， 只有当一个data node的所有数据块都在其他节点完成了备份以后， 该节点才能被移出集群，而由于数据备份是以较小的传输率运行在后台，往往要持续几个小时以上。

总而言之，传统大数据平台因为其结构性的缺陷导致了多租户环境下数据开发效率低、集群资源利用率不高、以及集成新技术很复杂等问题，依靠Hadoop生态技术框架本身的发展解决这些问题是比较困难的。

## 2.2 云原生大数据平台是大势所趋

既然不能够依靠Hadoop生态技术本身的发展来解决传统大数据平台带来的难题，那么我们就应该把注意力放到

当前最新的技术发展趋势之上，也就是以容器和K8s为代表的云原生技术。云原生技术在2013年容器项目以及2014年K8s项目正式发布以后，发展非常迅猛。现在，各大公有云厂商都支持K8s，还有上百家技术公司在持续投入K8s的迭代和更新工作。成立于2015年的云原生计算基金会（CNCF），将K8s作为其托管的第一个项目。截止2022年五月，该基金会已经托管了123个项目，近200个国家的16万开发者在为这些项目开发代码。更令人兴奋的是，CNCF的生态全景图目前包含了1000多个云原生技术产品，覆盖了数据库、消息级流处理、调度和任务编排、存储系统等10多个技术领域。

2021年应该是云原生大数据技术发展的里程碑，在这一年，有两个重大的技术进展被公布。一个是2021年3月，Apache 宣布 Spark 3.1 正式支持了 K8s， 另外在2021年5月， Apache Kafka 背后的商业公司 Confluent 也发布了Confluent on K8s，一个能私有发布的在K8s之上运行的Kafka生产集群系统。这两个重要事件表明，大数据平台的云原生化已是大势所趋。按照这个趋势，Hadoop也会逐渐迁移到K8s上。从技术角度来分析，常说的Hadoop三架马车中，计算框架MapReduce会被更高效的Spark所取代，资源调度组件Yarn正在被K8s取代，最坚挺的HDFS也有了云原生的对标方案。这意味着直接在K8s上运行所有现在的大数据工作负载已经成为了可能。

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-48-25.png" />

在最新的Data on Kubernetes 2022报告中，被采访的全球超过500家企业中，90%的企业认为K8s已经可以成熟运行有状态服务，超过70%已经在生产中运行数据处理相关的负载。这些企业将数据处理迁移到K8s的主要动机如下图所示：

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-48-45.png" />

随着K8s的进一步成熟和工具链的完善，我们相信越来越多的大数据应用会以云原生的方式发布。如何利用新的云原生体系提升大数据系统的效率，是每个希望高效完成数字化转型的企业需要回答的问题。

# 3 云原生大数据架构优势

## 3.1 提升资源使用率

传统大数据平台对于实时数据分析作业和离线数据分析作业是分开管理部署，各自采用独立的资源调度管理系统，比如离线作业使用Yarn调度，实时作业采用Flink自带的 调度机制。实时作业运行时间较长，对资源的需求存在周期性，比如白天使用资源较多，晚上使用资源较少，整体资源使用率不高。离线作业一般都是资源密集型的计算，比如大数据分析、机器学习模型训练等，但其有一定的运行周期，比如每小时或者每天运行一次，可以容忍较高的时延。实时和离线这两类作业的服务负载在分时复用、资源互补上存在极大的优化空间。但是传统大数据平台没有采用两种作业混部的方式，一是混部会带来底层共享资源（CPU、内存、网络、磁盘等）的竞争，会导致分析作业性能下降，而且这种下降是不可预测的，二是两种作业在传统大数据平台下无法使用统一的资源调度系统。

KDP以K8s技术为基座，将整个集群的资源形成了一个可以共享的资源池。同时，KDP实现了实时分析作业和离线分析作业的统一调度机制，比如Hive离线作业以Spark程序的方式运行，实时作业以Flink程序的方式运行，而Spark和Flink都采用Volcano来进行统一的精细化调度。在此基础之上，KDP实现了实时分析作业和离线分析作业在同一集群的混部，即同一个KDP节点上既可以运行实时分析作业，也可以运行离线分析作业。传统大数据平台的整体资源使用率一般只有30%左右，KDP通过作业混部能把集群资源使用率提升到60%以上。

部署在公有云环境下的KDP，通过云服务提供商提供的接口，可以实现对计算和存储资源的动态扩缩容。当集群需要更多资源的时候，KDP会启动更多的虚机加入集群，当用户资源使用率下降的时候，新分配的虚机资源可以被收回。通过动态扩缩容，系统有比较灵活的手段来应对短时间出现的性能瓶颈问题，而这些问题在传统大数据平台是难以应对的。在私有云环境下部署的KDP，通过虚拟化系统提供的接口也能实现一定程度的资源动态扩缩容，可以把备用的一部分虚机在不同的系统中进行共享，提高这部分虚机的资源复用率。

## 3.2 简化大数据组件集成及配置安装流程

在传统大数据技术架构下，大数据平台部署一直是一个复杂和费时的工作。每个大数据组件都有自己的部署方式和配置管理，大数据后台工程师需要了解和掌握10到20种Hadoop生态的技术，才能完成一个大数据平台的技术选型和部署准备工作。在部署过程中，大数据后台工程师除了要解决操作系统兼容性、大数据组件版本兼容性、软件包依赖、分布式集群网络配置等一系列问题，还要对各个大数据组件进行配置调整，以实现运行的稳定性、数据的安全性、监控指标的可观测性等生产级系统需要具备的能力。基于K8s技术构建的云原生大数据平台也存在类似问题，下面这张图是Data on Kubernetes社区在今年发表的报告中提到的一组数据，在调查了500多名公司高管和技术负责人以后，超过半数（53%）的受访人认为，在K8s中管理数据负载的最大挑战是缺乏自动化的大数据组件部署和配置管理。

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-49-12.png" />

为了应对这一挑战，智领云研发团队自主研发了KDP大数据集成基座，形成了自动化的、标准化的云原生大数据平台配置管理流程。该集成基座采用了统一的K8s文件配置方式对大数据组件的镜像文件、软件依赖、部署步骤、与K8s网络和存储的对接、计算和存储资源的配额、监控指标的输出、日志文件的采集等内容进行描述，然后安装程序会通过这些配置文件来自动执行大数据组件到K8s集群的部署、更新、运维、升级等操作。这样一个标准化的云原生大数据平台集成和配置流程有以下几点优势：

- 优势一：简化了大数据组件与K8s集群的集成。引入新的大数据组件时，只需要按照标准配置文件进行配置，在几个小时内就可以快速实现新的大数据组件部署到K8s。传统大数据平台引入一个新的大数据组件则需要几天甚至几周的时间。

- 优势二：降低了大数据平台部署的复杂性。一个后台工程师只需要掌握基本的K8s技术，就可以通过统一的部署方式进行大数据平台的发布，而不需要去学习和掌握10到20种不同的大数据技术。KDP的部署往往只需要几个小时就可以完成，而传统大数据平台的部署最少也需要一两天的时间。
- 优势三：减少了大数据平台运维的成本。一个拥有两百个节点的传统大数据平台往往需要5、6个全职大数据运维工程师进行运维，他们经常需要运行各种不同的命令对不同的大数据组件进行配置调优以解决各种性能问题。在KDP平台，这样规模的集群只需要1、2个懂K8s技术的运维工程师，通过统一的命令对各种大数据组件进行配置调优，而且这种操作的机会并不多，因为KDP平台本身会通过云原生技术自动解决大部分的性能问题。

## 3.3 提高计算作业的运行效率

传统大数据平台主要依赖Yarn来进行计算作业的调度，但Yarn有几个关键的技术局限性严重影响了计算作业的运行效率。首先，Yarn在调度过程中，是一种单线程的调度机制，随着集群规模增长和作业队列数目的增加，调度耗时越来越长。其次，当集群资源出现不足造成计算作业延迟完成的情况下，Yarn无法实现集群级别的资源扩展，比如说，自动加入一个计算节点。 另外，在多租户环境下， Yarn很难实现计算作业之间的隔离，很容易出现软件依赖故障，比如说，计算作业加载了错误版本的程序包造成某个类和方法找不到，解决这些问题需要对Yarn集群进行停机并做全局性的调整。

KDP的调度机制是采用Volcano作为一级调度和K8s作为二级调度来实现的，Volcano对K8s的缺省调度机制做了更精细化的管理，提供了更灵活的调度策略。Volcano可以在同一个K8s集群上部署多个实例，分别管理不同命名空间或者不同节点组（node group）的作业调度，所以基于Volcano的调度机制不会随着作业数目的增长而出现调度延时。 根据Volcano社区发布的技术文献，锐天投资公司的大规模分布式离线计算平台部署了多个 Volcano 实例，每天通过Volcano调度的作业有30万个。

KDP支持集群的弹性扩容，在集群的计算资源出现不足的情况下，利用K8s的弹性扩容技术来从容应对计算作业的性能瓶颈。首先，KDP对所有大数据组件都配置了基于Prometheus开源监控系统的监控指标，通过K8s的HPA（Horizontal Pod Autoscaling）API对象，KDP可以利用监控指标（CPU使用率等）自动扩容或者缩容大数据服务中的Pod数量，当计算负载增加时，KDP将自动增加大数据服务的Pod数量，提高系统稳定性，而当计算负载下降时，KDP将自动减少服务的Pod数量，减少对集群资源的请求量。另外，在公有云部署时，KDP可以配合公有云服务提供商的API接口，实现集群规模的自动伸缩，在需要计算节点的时候增加集群节点数，在集群计算负载下降的时候减少节点数，节省集群的整体成本。

最后，我们基于TPC-H测试框架的测试结果显示（详细结果请参见4.3节），在KDP上运行Spark，相比在传统大数据平台的Yarn上运行Spark，要节约5%左右的时间。经过调研发现，Yarn上运行的Spark程序在JVM GC上要花费更多的时间，频繁的GC会阻止Spark executor的进程从而影响整体计算性能。

总体上看，在计算作业的运行效率方面，KDP相比传统大数据平台具有明显的优势，因为KDP采用了灵活的、高并发的调度系统，支持计算资源以及集群资源的动态扩容，以及更好的计算性能。

## 3.4 降低运维成本并提升运维效率

传统大数据平台的运维成本是很高的，需要有熟练技能，了解和掌握各种大数据技术的运维工程师。下图是2018年根据当时美国薪资水平的计算出来的，一个公司每年大数据平台的运维人力成本在60万到120万美元之间。考虑到国内的人力成本会低于美国，折算一半的话一年也有200万到400万人民币的运维人力成本。

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-49-40.png" />

传统大数据平台运维的主要工作难度在于以下几点。首先是运维人员要掌握每个大数据组件独特的运维流程和运维命令，比如HDFS datanode扩容和Kafka broker扩容就是两个完全不同的操作方式，运维人员对这两个不同的技术都要有深入了解。其次，类似datanode和broker扩容这样的运维工作都比较耗时，运维人员首先需要申请资源，安装软件，然后进行配置优化，绝大部分工作都是通过各种脚本和命令进行人工操作。另外，运维人员所依赖的大数据日志系统在传统大数据平台下很复杂，需要在集群每个节点上部署和运维类似Flume的日志采集器，同时部署ElasticSearch这样的日志系统对日志进行存储和查询，而ElasticSearch本身也是需要不少资源，并存在一定运维复杂度的。

KDP大数据集成基座实现了大数据组件运维操作的标准化，通过K8s标准的operator操作方式完成大数据组件的部署、升级、扩容、备份等操作，熟悉K8s运维的工程师很快就能掌握这种运维方式。这种标准化的运维方式相比去掌握各种不同的大数据组件的运行原理和运维操作，其学习成本要低了很多。同时，KDP将一些基本的大数据组件运维操作集成到了KDP的运维管理界面，进一步提升了运维人员执行运维操作的效率。

由于KDP以容器化的方式运行大数据组件，同时又支持了大数据集群的动态资源扩容和集群规模扩容，所以类似datanode和broker扩容这样的运维操作就变得很简单了。比如说增加一个datanode到HDFS，如果不需要进行集群规模扩容，一个简单的operator命令就可以将datanode运行容器数增加一个，一个新的datanode实例就运行起来了，如果需要进行集群规模扩容，在公有云的部署方式下，运维人员也只需要在KDP中先运行一个新增节点的命令。

KDP的日志系统相比ElasticSearch是一个轻量级的系统，占用系统资源要少了很多。首先，日志的采集是通过为大数据组件的pod部署一个sidecar容器来实现的，这个比在各个节点上部署和运维一个Flume要简单了很多，因为这个sidecar的部署是标准化的大数据组件部署的一部分。其次，所有日志都是以流文件的形式推送和存储在Loki日志系统中，运维人员可以在Grafana界面上很方便地进行日志的浏览和查询，也可以利用KDP的logviewer服务将日志文件下载后进行查询。另外，Loki支持以对象存储的方式存储日志，借助K8s存算分离的技术，我们可以进一步降低日志系统的整体资源消耗成本。

# 4 KDP对大数据平台的云原生改造

KDP的主要功能是提供了一个标准的集成基座，将成熟的开源大数据组件以标准化的方式集成到K8s平台上，确保它们可以按云原生的方式部署，运行和运维。

## 4.1 K8s标准集成基座

目前，云原生开源社区已经提供了不少支持大数据组件部署的operator或者helm chart，比如Google非官方开源的Spark operator，Strimzi开源的Kafka operator，以及几个工程师一起开源的HDFS helm chart。但这些开源的operator和helm chart没有解决大数据组件在云原生环境下部署和运维的几个核心问题：

- 没有提供安全认证和鉴权管理；
- 没有提供对可观测性（监控、报警、日志）的支持；
- 没有提供对精细化调度系统的支持；
- 没有提供对多租户开发环境的支持，比如用户初始资源的创建等。

为了解决这些问题，智领云研发团队自主研发了KDP大数据集成基座，一是形成标准的云原生大数据组件集成流程，将开源大数据组件与统一系统服务（多租户、监控、报警、日志、网络、存储等）对接，形成标准化配置文件； 二是通过配置文件来完成大数据组件到K8s集群的发布、更新、运维、升级操作；三是提供大数据组件的可观测性服务，包括大数据组件以及其执行的workload的日志，性能及稳定性的指标监控和报警，计费以及审计功能；四是为计算大数据计算引擎提供云原生的调度机制支持，提升资源使用率与运行效率。

## 4.2 大数据组件的K8s改造

在标准集成基座的基础上，KDP集成了主流的大数据计算和存储引擎，所有大数据组件均以容器的方式运行在K8s系统之上。在下面的列表中，我们可以看到该平台所支持的大数据组件及其版本。在集成这些大数据组件的过程中，我们保留了其原生的访问方式（UI、接口、协议等），但是通过对开源代码的修改和扩展，强化了标准化的部署和运维、统一的可观测性、统一的安全认证和鉴权机制、以及性能上的优化等方面。

| 大数据组件            | 版本               |
| --------------------- | ------------------ |
| 分布式文件系统HDFS    | 3.1.1              |
| 分布式数据仓库Hive    | 3.1.3              |
| 分布式计算引擎Spark   | 3.3.0              |
| 分布式消息队列Kafka   | 2.8.1              |
| 分布式对象存储MinIO   | RELEASE.2022-09-07 |
| 批流一体计算引擎Flink | 1.14.6             |

相比传统的主流大数据组件，我们对Hadoop的支持只保留了HDFS，而没有选择其计算框架Yarn，一是因为Yarn在资源隔离方面的局限性，二是因为采用了K8s作为容器编排和调度引擎后，Yarn的存在显得有些多余，三是MapReduce计算引擎在性能上已经被Spark超越太多。为了支持Yarn所提供的各种调度策略，我们在计算引擎方面是支持云原生的精细化调度引擎Volcano，作为K8s之上的二级调度系统。保留HDFS的原因是因为目前还是有很多的企业在使用HDFS作为数据仓库的存储方案，在迁移到云原生系统以后，基于HDFS的这些作业可以直接迁移过来运行，而不需要重新开发。

KDP另一个技术特点是通过Spark计算引擎在K8s上运行Hive SQL。 Hive是深受众多大数据开发者喜爱的数据仓库开发工具，它使开发者通过简单易学的SQL语言进行大数据分析作业的开发和数据仓库的建设。以强大的Spark计算引擎来运行Hive SQL，则不仅仅是简化大数据分析的开发流程，同时也提升了大数据计算的运行速度。据我们了解， KDP是业界首个基于Spark计算引擎在K8s上运行 Hive SQL 的产品。 数据存储方面， 我们支持同时在HiveSQL中访问HDFS或者对象存储中的数据。也就是说，我们可以将热数据保存在HDFS中，冷数据保存在相对便宜的对象存储（比如MinIO）中，通过数据仓库工具Hive同时访问冷热数据。

## 4.3 组件K8s性能优化

虽然K8s对有状态服务的支持逐渐在加强，但是很多大数据组件在设计和实现时并没有针对云原生架构做相应的考虑，导致直接在K8s上运行时会有性能上的问题，或者是无法利用K8s的调度机制和资源管理来提升系统效率。因此，除了将大数据组件改造成能在K8s上运行之外，我们还要确保各种大数据组件在K8s上的运行性能，利用云原生的机制来提升组件性能，减少由于容器化和插件化带来的性能损耗，能够处理各种特性的大数据工作负载。例如，在性能优化方面，我们对Spark计算引擎做了相应优化，一是解决了云原生环境下的data locality问题，二是通过持续运行机制避免了Spark pod的频繁启动：

- Data locality问题就是尽量让Spark executor在其执行的节点本地获取数据，避免数据在网络上传输带来额外的性能损耗。我们对Spark driver的任务分配逻辑做了修改，使Spark在分配任务的时候会考虑到数据本地性，优先将任务分配给数据本地性最好的Executor执行。
- 持续运行机制就是在计算作业完成以后，仍然保存Spark driver和executor pod的运行状态，而不是在作业完成以后就让这些pod停止运行。为了避免这些持续运行的pod浪费计算资源，在指定时间（一般是30分钟）内如果这些pod没有运行任何计算作业，则它们会自动停止运行。

## 4.4 集成管理运维平台

对于集成的大数据组件，KDP利用K8s标准的发布和运维机制，为开发和运维人员提供了一个标准的运维管理平台Cluster Manager。类似于CDH的 Cloudera Manager，KDP允许用户在一个统一的界面里发布新的组件，查看组件运行情况，资源使用情况，运行日志，大大降低了运维的难度，提高了运维的效率。

与传统的集成管理运维平台相比，KDP完全使用云原生的方式安装，管理，运维底层的组件，支持租户资源配额管理和计费，因此，KDP Cluster Manager能够以标准化的方式添加新的组件，新添加的组件无需复杂的定制化和集成，就可以使用集成管理平台的各种运维工具，大大降低了集成新功能的成本。

KDP Cluster Manager的主要功能包括：

- 集群管理
- 应用管理
- 租户管理
- 资源管理
- 系统配置管理
- 监控报警管理

# 5 KDP系统架构及功能组件

## 5.1 系统架构

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-50-26.png" />

从架构上看，智领云研发团队自研的部分主要是我们前面提到的大数据集成基座，以及可观测性服务的异常检测、计费、审计和血缘功能。同时，智领云研发团队对主流的大数据计算引擎、存储引擎、调度引擎和开发工具进行了源代码级别的修改，主要的修改集中在以下几个方面：

- 支持K8s集群环境下生产级别的运行稳定性及高可用；
- 优化计算引擎Spark的性能，包括实现Data Locality和计算pod持续运行；
- 支持统一的调度机制以及精细化的作业调度策略；
- 支持可观测性服务的适配，包括监控、报警、日志、审计、计费等；
- 实现基于OpenID协议和开源软件Keycloak的身份认证和访问控制方案;
- 支持基于Kerberos协议的安全认证和基于Apache Ranger的统一授权管理框架；
- 支持以标准化（operator或者helm chart）的集群部署方式进行适配；
- 实现与BDOS云原生大数据集成基座的适配，实现可视化的发布、配置修改、运维等。

## 5.2 集成框架

KDP的集成框架提供了一个标准集成流程，允许各种大数据组件以标准化配置文件的形式与统一系统服务对接，形成标准化部署，运行和运维流程。它在K8s配置的基础上提供封装，简化大数据组件的配置流程，标准化组件与系统服务及其它组件之间的对接机制。

KDP集成框架的主要功能有：

- 将开源组件的发布运维流程以标准配置文件方式集成；
- 提供灵活的发布配置管理，例如允许用户指定namespace发布，以及依赖组件的指定；
- 如有依赖外部系统，可从系统变量中读取依赖系统访问地址及配置；
- 标准化ConfigMap, secret等系统配置组件的发布和配置方式；
- 提供日志，监控报警等运维插件配置，隐藏底层系统细节，自动化，简化配置；
- 系统配置的版本管理，对比，回滚等功能。

详细介绍：

KDP的集成框架是一个基于Kubevela构建的创新性工具，旨在为大数据组件的部署和管理提供一套标准化、简化的流程。整个框架以K8s配置为基础，通过封装和抽象，实现了对各种开源大数据组件的统一管理。它包括三个关键部分：预定义traits、三层配置文件、大数据组件的部署文件。

首先，通过Kubevela构建的集成框架将各种大数据组件的operator和helm charts进行了抽象，使它们以标准化的方式进行部署。这为不同的大数据组件提供了一致的部署体验，使整个流程更加规范和易于管理。其次，通过预先定义的方式，大数据组件所需的通用配置、监控指标输出、监控面板定义、以及日志sidecar配置被定义为Kubevela traits。这些traits在大数据组件安装之前就会被提前部署，为后续的组件集成提供了标准的基础配置，极大地简化了组件的部署和对接过程。 

框架的另一个重要特性是提供了多层级的配置文件，包括全局、集群和组件三层配置文件。在这些配置文件中，可以灵活地设置组件部署中所需的参数，为用户提供了更加个性化的配置选择。这种多层级的配置方式既满足了全局性需求，又为特定组件的个性化配置提供了便利。此外，框架还标准化了定义ConfigMap、Secret、PV等K8s资源的方式，使得这些资源能够在整个集成流程中被一致地管理和调用。通过在配置文件中配置namespace，用户能够灵活指定大数据组件发布的namespace，增强了对组件的隔离性和灵活性。最后，每个大数据组件只有一个标准的部署文件，这一设计在很大程度上简化了大数据组件的部署定义。通过集成框架，用户无需为每个组件编写复杂的部署文件，而是采用统一的方式，使整个部署过程更加高效和易于维护。 

总体而言，KDP的集成框架以其标准化、简化和灵活性的特点，为大数据组件的部署和管理提供了创新性的解决方案，使整个流程更加规范、高效，为用户提供了便捷的使用体验。

## 5.3 系统发布服务

KDP的系统发布服务负责大数据组件从配置文件到K8s集群的发布，更新，运维，升级操作。和通用的PaaS平台应用管理功能相比，KDP的系统发布功能需要支持大数据类型的负载（比如大量即起即停的批处理任务），与大数据租户体系，权限管理，用户管理，其资源管理的对接和集成。

KDP系统发布服务的主要功能有：

- 实现infra-as-code的发布运维方式，所有操作以修改配置文件的方式并以control loop的方式实现
- 负责有依赖关系的组件之间的发布流程，无需手动处理
- 系统组件和租户体系的集成，确保授权，鉴权，权证的发布以云原生的方式完成
- 租户的管理，机构/用户以及其相关资源的生命周期管理
- 根据运行负载情况实现动态扩容降容

详细介绍：

KDP的发布服务在大数据组件的发布、更新、运维和升级方面发挥着关键作用。相较于通用的PaaS平台应用管理功能，发布服务具备独特的特点和功能：

首先，它实现了infra-as-code的发布运维方式，将所有操作都纳入配置文件的管理范畴，并通过K8s control loop的方式来实现。这意味着用户可以通过简单地修改配置文件来执行各种操作，实现了对基础设施的代码化管理，为整个大数据组件的生命周期提供了可控的、可追溯的操作方式。其次，发布服务构建了一个包含K8s控制面板、容器镜像仓库、Artifacts服务器等环境配置的shell环境。这为用户提供了一个一体化的操作环境，使得大数据组件的发布和管理变得更加直观和高效。通过这个环境，用户可以轻松地配置和管理与大数据组件相关的各种资源。同时，发布服务采用了以shell命令行的方式进行大数据组件的发布。这种直观的命令行操作使用户能够快速、灵活地执行发布任务，降低了学习曲线，同时为运维人员提供了更为便捷的方式来处理各类操作，从而提高了整体的运维效率。

总体而言，KDP的发布服务通过实现infra-as-code、构建一体化的shell环境和采用命令行发布方式，为大数据组件的发布和管理带来了新的思路和便利。这些特点使得整个发布流程更为可控、直观，为用户提供了更高效的运维体验。

## 5.4 计算引擎云原生改造及集成

### 5.4.1 Apache Hive

Hive是构建在Hadoop之上进行数据查询和分析的数据仓库工具。Hive的元数据管理将HDFS或者（支持S3协议的）对象存储上存储的结构化文件定义成数据库中的表，同时，Hive提供类似SQL的数据库查询语言来查询存储在HDFS或者对象存储上的结构化文件。 在KDP， Hive最显著的提升是Hive Metastore和Hive Server2实现了高可用和负载平衡。 将Hive的组件容器化了以后， Hive Metastore可以轻松实现高可用，并且可以部署不同版本的Hive Metastore以支持不同版本的Hive，实现在同一集群同时运行legacy和新的Hive SQL代码。同时我们可以部署多个Hive Server2实例将Hive作业的请求进行分流，避免Hive Server2成为提交大量Hive作业的性能瓶颈。

KDP在云原生环境下为用户提供以下支持：

- Hive查询作业将会转化成Spark作业在K8s中运行。
- KDP支持两种方式提交Hive SQL作业，一种是通过数据查询工具Hue，另一种是通过Beeline客户端。
- Hive的数据存储支持云原生部署的HDFS和对象存储，也支持平台外部部署的数据存储。

### 5.4.2 Apache Spark

Spark是一个支持批处理和流处理的分布式计算引擎，它可以运行在YARN、Mesos和K8s这些调度机制之上，可以处理HDFS、对象存储等多种文件系统上的数据。Spark是基于内存的计算，所以它的计算速度比一般的MapReduce作业要快。

- KDP通过operator的方式部署Spark on K8s。
- 提供HTTP API的方式提交Spark作业， 也支持标准的作业提交方式， 比如通过交互式编程工具JupyterLab运行Spark代码，或者以Hive SQL的方式运行Spark作业。
- KDP解决了Spark on K8s的Data Locality问题并实现了driver和executor pod的持续运行，极大地提升Spark在K8s上的性能。

### 5.4.3 Flink

与Spark一样，Flink 也是一个支持批处理和流处理的分布式计算引擎，它可以运行在YARN、Mesos 和K8s这些调度机制之上，也是基于内存的计算。不同之处在于，Flink 的流处理是把输入流当作无边界的输入来处理，而批处理则是特殊的流处理，它处理的是一段有边界的流数据。Flink 的流处理具有高吞吐、低延迟、高性能的特性。

- KDP基于社区版的 Flink operator 对 Flink 进行了云原生的改造， 社区版的 Flink 不支持调度引擎，KDP则将Spark和Flink都纳入了统一的调度引擎。
- KDP实现了Flink跟其他云原生大数据组件的对接，以及社区版Flink operator在生产环境中出现的影响稳定运行的一系列问题。

### 5.4.4 Kafka

Kafka是一个分布式的流处理平台，具备高吞吐量、高可用、无延误等特点，主要用于大数据实时处理。KDP对Kafka的云原生改造主要是以下两点：

- 强化了数据安全管理，实现了基于Kerberos的安全认证和基于Apache Ranger的授权管理。
- 在Kafka operator中集成了Kafka管理工具Kafka manager和Schema Registry插件的部署，前者可以通过界面对Kafka集群和消息进行管理，后者可以支持Avro格式的消息在Kafka中进行生产和消费。

## 5.5 存储引擎云原生改造及集成

### 5.5.1 HDFS

分布式文件系统HDFS是一个可以运行在普通硬件上的文件系统，它通过将文件分块保存到不同的节点并且每个块保存多个副本的方式来实现高容错性。HDFS存的每个数据块有3个备份，分布在不同的数据节点上，来实现高可用。KDP通过对开源的helm chart进行扩展将HDFS的非云原生特性进行了改造：

- 原来的基于本地硬盘的存储改造成了基于PV的云原生存储模式。
- 将host网络改造成了pod的虚拟网络。
- 实现了datanode的（在硬件环境支持的情况下）弹性扩容。

原本基于本地硬盘的存储方式被KDP改造成了基于PV的云原生存储模式。这一改进提高了数据的可靠性和可扩展性，使HDFS更适应云环境的动态特性。其次，将原有的基于host网络的通信模式转变为pod的虚拟网络。这种改造增强了网络的隔离性，使得HDFS更好地适应容器化环境，提高了整体的可管理性和灵活性。在实现高可用方面，引入了基于Zookeeper的高可用部署模式。这个改进确保了HDFS在面临节点故障时能够保持高可用性，为用户提供了更加可靠的服务。安全方面，通过集成Kerberos安全认证和Apache Ranger授权机制，KDP实现了更加严密的安全层。这为敏感数据的保护提供了可靠的手段，增强了HDFS在企业环境中的可信度。KDP还实现了企业级的多租户安全和访问机制，使得不同租户之间的数据得到隔离，确保数据的安全性和完整性。最后，通过与Prometheus和Grafana的集成，KDP构建了全面的监控、报警和日志体系。这使得用户能够及时了解系统运行状况，提高了对系统性能和问题的实时响应能力，为HDFS的运维提供了更全面的支持。

综合而言，KDP通过深度改造HDFS Helm Chart，使得HDFS在云原生环境中更加灵活、可靠、安全，并提供了全面的监控和管理手段，为大规模分布式存储系统的部署和运维带来了显著的优势。

为了保障HDFS系统的稳定性和安全性，与多个关键的大数据组件协同工作，以配合HDFS的部署和运维。这些主要的组件包括zookeeper、kerberos、ranger、prometheus、grafana和loki，各自担负着不同的关键任务，共同构建了一个强大而全面的大数据生态系统。这些大数据组件的有机协同，不仅保障了HDFS的高可用性和安全性，还提供了全面的监控和日志管理手段，为大规模数据处理提供了强有力的支持。

其中Zookeeper在系统中的作用是实现双Namenode的高可用部署。通过Zookeeper，系统能够确保在主Namenode出现故障时，能够平滑地切换到备用Namenode，从而实现系统的高可用性。这对于保障数据的连续性和可靠性至关重要。其他的组件将在后续章节介绍。

### 5.5.2 对象存储MinIO

MinIO是一个开源的对象存储引擎。对象存储可以用标准的s3协议来访问，其特点是文件都是以对象的方式存储，没有目录的概念，可以存储大量的非结构化文件。通常部署在云端，采用比较便宜的存储介质。各大云厂商都有对象存储的实现。MinIO作为一种企业级的对象存储系统，支持私有化发布，它通过纠错码的方式实现数据冗余，能够避免出现单点失效。通过合理的配置，MinIO的有效存储空间可以达到75%以上。KDP基于开源的Helm chart进行了扩展：

- 支持在大数据集成基座中对MinIO的用户和权限进行统一管理
- 支持在Hive中同时访问HDFS和MinIO，也就是实现了在HDFS中存储热数据，而在MinIO存储大量的冷（历史）数据。

### 5.5.3 物理存储

对于存储引擎背后的物理存储，KDP是通过声明PVC，以PV的方式来实现的。由于HDFS、MinIO和Kafka本身的存储架构已经实现了数据存储的高可用，那就没必要在物理存储层再做高可用的实现，所以KDP采用openEBS的Local PV来做这些存储引擎和Kafka的物理存储， 当少数节点或者PV出现故障的时候， 并不会影响这些存储引擎和Kafka的可用性。我们的性能测试指标显示，这种Local PV的存储方案相比直接磁盘访问，并没有显著的性能差异。

## 5.6 云原生多租户和安全管理

### 5.6.1 多租户管理和资源隔离

KDP通过大数据集成基座进行多租户管理。每个新用户都会创建单独的用户账号和对应的Kerberos keytab，并加入相应的安全组。每个安全组都有自己独立的K8s命名空间，每个命名空间都有对应资源配额，每个安全组的计算作业都会发布到各自的命名空间。KDP通过这种方式实现了多租户的用户管理和资源隔离。

### 5.6.2 Keycloak身份认证和访问控制

Keycloak是一个开源的身份认证和访问控制软件，它通过对各种单点登录协议的支持来完成对不同系统的单点登录。KDP通过对开源大数据工具的登录方式进行扩展，基于OpenID协议在Keycloak上实现了这些大数据工具的单点登录，用户使用同一个账号即可登录所有大数据工具。

- KDP以容器化的方式运行Keycloak，将其元数据保存在分布式存储中，这样就保证了Keycloak的高可用。
- BDOS的大数据集成基座在创建用户和安全组的时候，会自动在Keycloak中创建账号，在部署大数据组件时会自动在Keycloak中创建相应组件的客户端。KDP支持单点登录的大数据工具有数据分析工具Hue、交互式编程工具JupyterLab和交互式BI报表工具Superset。

### 5.6.3 Kerberos安全认证

Kerberos是一个网络认证协议，它提供一种可信任的第三方认证服务，通过对称加密的方式为服务器/ 客户端应用提供验证服务。Kerberos 协议在大数据系统中被广泛用来保证数据和服务的安全性。

- KDP以容器化的方式部署了Kerberos主服务KDC，通过分布式存储保存KDC的源数据来实现KDC的高可用。
- KDP以K8s secrets的方式保存大数据服务的keytab，以分布式存储的方式存储用户的keytab，实现了Kerberos服务的完全云原生化。

### 5.6.4 Apache Ranger授权管理

Apache Ranger是一个对大数据资源的使用权限进行集中式配置、 管理和监控的框架。 KDP通过对开源大数据组件源代码的扩展，以统一的方式实现了HDFS、Hive、和Kafka 组件的权限访问控制：

- 在Apache Ranger的管理界面可以针对用户或者安全组， 对HDFS的目录、 Hive中的数据表、 以及Kafka中的topic进行权限访问的控制。

- BDOS大数据集成基座在创建新的用户和安全组的时候，会自动在Ranger中自动创建相应的安全策略。

- 元数据的存储方面，跟其他大数据组件类似，KDP以云原生方式实现了Ranger的元数据存储，实现了服

  务的高可用。

## 5.7 可观测性服务

KDP可观测性服务提供大数据组件以及其执行的workload的日志，性能/稳定性的指标监控和报警，计费以及审计功能。除了提供接口和SDK方便大数据组件接入可观测性服务，KDP可观测性服务和通用PaaS平台最大的区别在于需要支持：批处理任务，二级调度任务，各种数据层面的指标。

### 5.7.1 Prometheus和Grafana

Prometheus是一个用于监控和报警的开源软件，它通过服务发现和静态目标配置，实现对监控目标对象的监控指标的收集，同时还提供了一个灵活的查询语言来查询多维数据。Grafana是一个对时序数据进行查询和可视化分析的开源软件，它经常与Prometheus一起使用，把Prometheus 收集到的监控数据以直观漂亮的图形展示。

- KDP实现了Prometheus和Grafana的容器化发布
- KDP通过大数据集成基座，实现了大数据组件部署发布流程与监控报警系统Prometheus和Grafana进行自动地对接。

### 5.7.2 日志聚合工具Loki

Loki是可以跟Grafana集成在一起的日志聚合工具，它提供高效保存和查询日志的数据存储。

- KDP的大数据集成基座为每个大数据组件的pod都配置了一个sidecar容器，容器中运行Promtail代理程序。Promtail代理程序专为Loki而设计，它获取大数据组件容器的日志，将日志转换为流，然后通过HTTP API将流推送到Loki。
- KDP除了在Grafana中配置了日志查询界面，同时也提供自研的logviewer服务，通过接口的方式获取大数据组件的日志。

### 5.7.3 计费和审计功能

KDP的计费功能是收集大数据计算引擎和存储引擎的资源使用数据， 对其进行整理， 存储和分析， 并提供HTTP API， 让用户能清晰地看到各安全组， 各用户， 各计算作业等的资源使用情况。 BDOS的运维管理界面提供审计功能，让用户能够对资源使用情况进行查询和审计。

# 6 系统部署及性能

## 6.1 公有云和私有云部署

KDP可以在公有云平台部署， 已完成与阿里云容器服务、 腾讯云TKE服务、 华为云CCE服务、 亚马逊EKS服务的适配，包括在托管K8s服务是那个部署平台、采用这些公有云平台的云原生存储方案、以及对接云原生计算或存储组件等。KDP也支持私有云部署，目前已完成与CentOS 7.9操作系统、社区版K8s和OKD上的部署适配。

## 6.2 国产软硬件支持

目前社区版K8s已经完成与中标和银河麒麟操作系统，以及基于飞腾和鲲鹏芯片的国产Arm架构CPU服务器的适配。而KDP可以基于社区版K8s部署，所以理论上是可以在社区版K8s所支持的国产软硬件上进行部署的，在落地实施的实践中，目前KDP已经完成了与中标和银河麒麟操作系统，以及华为、浪潮、新华三等多家国内服务器的软硬件适配。

## 6.3 性能对比

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-53-17.png" />

### 6.3.1 部署效率

- 时间（少）：小时级别系统应用部署
- 依赖复杂度（低）：基于集成框架的标准化配置管理，解决版本兼容、软件包依赖及分布式网络配置等一系列难题
- 操作难度（低）：界面化操作，自动化部署，全程可观测

### 6.3.2 资源效率

- 资源利用率（高）：从30%提升到60%及以上
- 资源统计颗粒度（细）：除了集群总资源消耗统计外，细化到对各个namespace、各个应用负载、各个计算作业的资源统计与聚合
- 资源管理范围（广）：除了对集群、节点资源进行管理外，还对各个namespace、各个应用实例等的资源进行自助式配置管理
- 资源共享程度（高）：形成一个可以共享的资源池，计算、存储资源可动态调整，解决由于计算、存储资源独立分配而导致的资源利用低的问题

### 6.3.3 运维效率

- 运维人数（少）：一个后台工程师只需要掌握基本的K8s技术即可进行大数据组件运维
- 运维门槛（低）：统一界面操作及自动化运维
- 问题定位与排查（快）：全局可观测性服务，自动监控与告警，统一日志管理界面

### 6.3.4 HDFS读写效率

在 KDP 的研发和测试过程中，在腾讯云容器服务TKE上进了HDFS的性能测试，为云化HDFS提供了有益的洞察。首先，我们使用16台配置相同的工作节点，每个节点具有Centos7.6-64bit操作系统、16个CPU核心、32GB内存以及3TB存储。测试的目标是比较传统HDFS集群和在K8s环境中部署的HDFS集群在读写性能方面的表现。

测试过程首先通过传统方式部署了一个HDFS集群，然后在K8s环境（版本号1.22.5）中进行了相同配置的HDFS集群部署。读写性能测试选择了一个大小为17GB的TPCH标准测试集中的orders.tbl文件，并通过在非datanode节点上执行读写操作进行性能对比。在读取测试中，通过将读取结果重定向到/dev/null，不写入本地磁盘来评估文件读取速度，命令为：`time hdfs dfs -cat /tpch/orders/orders.tbl > /dev/null`。在写入测试中，将本地文件写入HDFS中，命令为：`time hdfs dfs -put /tpch/orders/orders.tbl > /tmp/`。通过多轮测试，最后选取10轮测试的平均值。

| 测试文件大小：17GB | 传统HDFS集群 | K8s HDFS集群 |
| ------------------ | ------------ | ------------ |
| 读（平均值）       | 313.12MB/s   | 305.46MB/s   |
| 写（平均值）       | 85.13MB/s    | 82.76MB/s    |

总体结论表明，云化HDFS相对于传统HDFS，在性能上的损耗约为2.5%。这一结果意味着在K8s环境中运行的HDFS集群在读写性能方面表现良好，与传统部署方式相比性能损耗较小。这是一个积极的发现，说明了在容器化环境中运行HDFS的可行性，并为企业提供了更加灵活和高效的数据存储解决方案。

需要注意的是，性能测试的结果可能受到多种因素的影响，包括网络延迟、存储设备性能以及K8s的调度策略等。因此，在实际生产环境中，建议继续监控和优化系统以确保最佳性能。此测试报告为云化HDFS在K8s上的性能表现提供了基础数据，为进一步优化和部署提供了有力支持。

# 7 产品安全性

产品安全的目标是要维护信息资源的保密性、完整性和可用性，以确保产品提供的业务能正常有序地运作，维护用户的合法权益。KDP采用威胁建模、安全设计、安全实现、安全测试等流程来确保产品安全从设计阶段、实现阶段到交付阶段的稳定落实。

## 7.1 威胁建模

产品在设计环节就针对常见的Web威胁进行了威胁建模，并针对性地构建了安全解决方案，已经划入范围的Web威胁包括但不限于：

- 默认弱口令
- 暴力破解
- SQL注入
- 跨站点脚本（XSS）
- 跨站点请求伪造（CSRF）
- 信息泄露
- 不安全的身份认证
- 不安全的访问控制

## 7.2 常见威胁的针对性解决方案

- 默认弱口令：对于默认的管理账号的口令存在部署后未及时修改的问题，针对此问题，根据内部相关文件指示，对于默认账号的口令进行了加固，采用长度不少于8位、包含大写字母、小写字母、数字及特殊字符、不含有连续字符（如123456、abcde等）、不含有键盘排序特征（如qwer、qaz等）的字符串作为默认账号的默认口令，即使在部署后未能及时修改默认管理账号的口令，也能免于常见的针对弱口令的攻击；
- 暴力破解：针对暴力破解攻击，采用了一定时间内登录错误次数累计超过阈值时触发锁定账号的安全机制，并能产生、留存相关的日志以供审计追溯使用；
- SQL注入：根据内部相关安全文档指示，采用了后台校验参数、采用安全API等方式进行安全实现；
- 跨站点脚本：根据内部相关安全文档指示，采用了输入前后台校验、限制输入长度、对特定字符进行转义、为重要cookie设置httpOnly等方式进行安全实现；
- 跨站点请求伪造：根据内部相关安全文档指示，对首部的referer字段进行了检查，并在敏感业务的表单中添加token以防御该攻击
- 信息泄露：针对该问题采用的方案包括但不限于：采用https协议部署站点；不在cookie中存放敏感数据；需要通过表单传输用户密码时采用不可逆的hash算法处理用户密码；用户密码在服务器端采用安全的hash加盐的方式处理后存储；
- 不安全的身份认证：针对该问题采用的方案包括但不限于：不在url中暴露sessionId；用户登录、用户退出都更换sessionId；
- 不安全的访问控制：针对该问题采用的方案包括但不限于：采用安全的JSON Web Token实现控制访问；除公开资源外的资源默认拒绝访问；  

## 7.3 安全实现与安全测试

KDP实现环节会根据设计阶段提出的安全实现方案进行编码实现；在产品测试环节采用了代码审计和安全测试相结合的手段，代码审计采用SAST和DAST工具对代码进行审计，并集成到CI/CD过程中，能够对现有的和新合入的代码中的某些安全风险（如SQL注入、XSS等）实现预警，安全测试分为自动化测试和手工测试两部分，自动化测试采用知名安全厂商出品的自动化测试工具进行安全测试，手工测试则是根据产品的具体实现由安全测试人员编写定制化的安全测试用例并手工执行，在产品测试环节确保产品实现环节按照预定的安全方案进行了安全实现。

## 7.4 安全保障

除了以上环节确保产品安全的落实以外，还针对规范要求的其他安全需求进行了相应的实现。

- 数据加密：用户密码是采用hash加盐的方式处理后存在数据库中的，不会出现泄露明文用户密码的问题；针对KDP中的敏感数据都采用了合适的加密手段进行了脱敏、加密的处理；
- 日志审计：实现了丰富的日志功能，确保每个操作都能被审计追溯，尤其是针对敏感数据的操作和一些重要业务，比如针对登录功能中对暴力破解的防护，能够清楚地记录下攻击的账号、攻击发生的时间、攻击发生的频次等信息。

# 8 应用场景

KDP实现了在K8s环境下，高效、安全、稳定地运行主流的大数据计算及存储引擎，通过标准的大数据集成基座完成大数据组件的部署、配置和运维，是多租户环境下大数据开发的最佳技术选择。特别是在以下大数据技术落地实施的具体场景中，该平台能很好取代传统大数据平台，帮助企业在数字化转型过程中实现降本增效的目标。

## 8.1 高效的集群部署和运维

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-54-15.png" />

有的企业作为技术提供方要为多个内部或外部的机构进行大数据集群的部署和实施，但传统大数据平台的软件部署、组件互相适配、计算引擎调优等方案相对复杂，手工部署的步骤多，导致集群部署周期长，项目实施成本很高，运维流程复杂，运维人员能力要求高。在这种场景下，采用KDP，可以大幅度提升实施项目的部署效率，降低项目实施运维人力和时间成本。

## 8.2 提升IT架构资源效率

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-54-36.png" />

有的企业在生产环境中运行多种类型的数据应用、不同类型的存储引擎、实时和批处理的计算作业。在传统大数据平台环境下，一般都是采用独立的虚机集群来部署这样的生产环境，导致资源使用率很低。采用了KDP以后，企业可以利用作业混排、存算分离和精细化调度等平台特性来提升整体资源使用效率，降低IT架构的投入成本。

## 8.3 传统技术的升级改造

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-54-55.png" />

传统大数据平台因为技术扩展迭代流程比较慢，不能及时解决运维中碰到的性能瓶颈，同时大数据组件之间软件包依赖很复杂，导致组件升级困难，新的组件集成耗时费力。使用传统大数据平台的技术团队面对运维压力疲于奔命，没有精力专注于业务开发和数据价值的发现。传统大数据平台逐步迁移到云原生大数据平台后，可以显著提升运维效率，降低运维成本，解放技术团队的生产力。

## 8.4 自助式数字创新

<img src="./KDP-WhitePaper/KDP-WhitePaper-2023-09-22-17-55-14.png" />

有的企业需要有多个大数据集群服务不同的业务部门，业务部门的数据科学家希望能自助式地尝试新的云原生人工智能机器学习工具。很显然，传统大数据平台满足不了这种自助式需要，企业可以通过KDP部署提升多平台管理效率，提供数据分析和人工智能开发工具的自助式发布，降低整体资源消耗的成本，加速数据价值的创造过程。

# 9 总结

KDP是智领云研发团队基于容器、K8s和大数据技术研发的大数据平台产品。这一产品的诞生，是要利用云原生的资源隔离、作业混排、存算分离、标准化部署运维等技术优势，解决传统大数据平台因本身技术局限性而无法解决的资源利用率低、部署运维复杂、难以弹性扩容等技术难题，帮助用户花更少时间、用更少的资源去进行大数据平台的部署、配置和运维，投入更多的时间和精力去进行大数据开发和分析，更高效、更稳定地从海量数据当中挖掘数据的价值并提升企业的数字化能力。

KDP支持主流的大数据计算和存储引擎，强化了这些大数据组件在生产环境下的安全性、稳定性和运行性能。同时，平台的大数据集成基座实现大数据组件的标准化部署和运维，统一了所有大数据组件与可观测性服务的适配，并且支持了在K8s调度机制之上的更精细化的作业调度机制。平台支持在公有云和私有云部署，支持社区版的K8s和主流的商业版K8s，适用于不同的企业环境。

智领云相信云原生技术是企业数字化转型的必经之路，云原生大数据平台则是取代传统大数据平台的必然结果。智领云研发团队将会密切关注云原生技术和大数据技术的发展路线，在大数据技术的云原生化方向为企业提供更好的产品和技术服务，帮助企业打造面向未来的云原生数字底座。
